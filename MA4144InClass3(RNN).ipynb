{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda0e25f-476a-43c0-b3e7-f96a707be566",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# <center>Recurrent Neural Networks</center>\n",
    "## <center>Inclass Project 3 - MA4144</center>\n",
    "\n",
    "This project contains multiple tasks to be completed, some require written answers. Open a markdown cell below the respective question that require written answers and provide (type) your answers. Questions that required written answers are given in blue fonts. Almost all written questions are open ended, they do not have a correct or wrong answer. You are free to give your opinions, but please provide related answers within the context.\n",
    "\n",
    "After finishing project run the entire notebook once and **save the notebook as a pdf** (File menu -> Save and Export Notebook As -> PDF). You are **required to upload both this ipynb file and the PDF on moodle**.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03930b71-f7aa-4eb8-a078-70fc89a1ac16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Outline of the project\n",
    "\n",
    "The aim of the project is to build a RNN model to suggest autocompletion of half typed words. You may have seen this in many day today applications; typing an email, a text message etc. For example, suppose you type in the four letter \"univ\", the application may suggest you to autocomplete it by \"university\".\n",
    "\n",
    "![Autocomplete](https://d33v4339jhl8k0.cloudfront.net/docs/assets/5c12e83004286304a71d5b72/images/66d0cb106eb51e63b8f9fbc6/file-gBQe016VYt.gif)\n",
    "\n",
    "We will train a RNN to suggest possible autocompletes given $3$ - $4$ starting letters. That is if we input a string \"univ\" hopefully we expect to see an output like \"university\", \"universal\" etc.\n",
    "\n",
    "For this we will use a text file (wordlist.txt) containing 10,000 common English words (you'll find the file on the moodle link). The list of words will be the \"**vocabulary**\" for our model.\n",
    "\n",
    "We will use the Python **torch library** to implement our autocomplete model. \n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db6bc0-f7e0-473d-a172-e6579deea2ee",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the below cell to use any include any imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fdc286-3211-4a8f-9802-29d28a324bea",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622b61b-dba8-47bb-8e07-92b77e78f4fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 1: Preparing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555a82e5-e56c-4075-a2a2-071633cd4d4c",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "WORD_SIZE = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f44ef-91d0-4d0e-afb5-a66240c9e1d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Instructions"
   },
   "source": [
    "**Q1.** In the following cell provide code to load the text file (each word is in a newline), then extract the words (in lowercase) into a list.\n",
    "\n",
    "For practical reasons of training the model we will only use words that are longer that $3$ letters and that have a maximum length of WORD_SIZE (this will be a constant we set at the beginning - you can change this and experiment with different WORD_SIZEs). As seen above it is set to $13$.\n",
    "\n",
    "So out of the extracted list of words filter out those words that match our criteria on word length.\n",
    "\n",
    "To train our model it is convenient to have words/strings of equal length. We will choose to convert every word to length of WORD_SIZE, by adding underscores to the end of the word if it is initially shorter than WORD_SIZE. For example, we will convert the word \"university\" (word length 10) into \"university___\" (wordlength 13). In your code include this conversion as well.\n",
    "\n",
    "Store the processed WORD_SIZE lengthed strings in a list called vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3219551c-a298-424a-a491-2d7f65a4ad6f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "\n",
    "with open(\"wordlist.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        word = line.strip().lower()\n",
    "        if 4 <= len(word) <= WORD_SIZE:  \n",
    "            padded_word = word.ljust(WORD_SIZE, '_')  \n",
    "            vocab.append(padded_word)\n",
    "\n",
    "# print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3a6fe-c0a7-4808-aa1a-4c3ad6db6de3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font color='blue'>In the above explanation it was mentioned \"for practical reasons of training the model we will only use words that are longer that $3$ letters and that have a certain maximum length\". In your opinion what could be those practical? Will hit help to build a better model?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74663988",
   "metadata": {},
   "source": [
    "**Answer** (to write answers edit this cell)\n",
    "\n",
    "1. Very short words does not contain enough information and also they do not need to be predicted since there are less than 3 number of characters. And the ambigiouty is high when the model tries to predict words by looking at only 1 or 2 characters. \n",
    "\n",
    "2. Fixed-length sequences make batching and training much more efficient.\n",
    "\n",
    "3. To have too long words are also unnecessary since the use case of those words are very limited compared to the common words. Therefore using computation ppower to train the model for such rare cases are not necassary since they take both memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b2e6d-f771-4782-8b21-73c121565faa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Q2** To input words into the model, we will need to convert each letter/character into a number. as we have seen above, the only characters in our list vocab will be the underscore and lowercase english letters. so we will convert these $27$ characters into numbers as follows: underscore -> $0$, 'a' -> $1$, 'b' -> $2$, $\\cdots$, 'z' -> $26$. In the following cell,\n",
    "\n",
    "(i) Implement a method called char_to_num, that takes in a valid character and outputs its numerical assignment.\n",
    "\n",
    "(ii) Implement a method called num_to_char, that takes in a valid number from $0$ to $26$ and outputs the corresponding character.\n",
    "\n",
    "(iii) Implement a method called word_to_numlist, that takes in a word from our vocabulary and outputs a (torch) tensor of numbers that corresponds to each character in the word in that order. For example: the word \"united_______\" will be converted to tensor([21, 14,  9, 20,  5,  4,  0,  0,  0,  0,  0,  0,  0]). You are encouraged to use your char_to_num method for this.\n",
    "\n",
    "(iv) Implement a method called numlist_to_word, that does the opposite of the above described word_to_numlist, given a tensor of numbers from $0$ to $26$, outputs the corresponding word. You are encouraged to use your  num_to_char method for this.\n",
    "\n",
    "Note: As mentioned since we are using the torch library we will be using tensors instead of the usual python lists or numpy arrays. Tensors are the list equivalent in torch. Torch models only accept tensors as input and they output tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "054a4ab4-5883-4948-adc5-eb8916b6234d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def char_to_num(char):\n",
    "\n",
    "    if char == '_':\n",
    "        return 0\n",
    "    \n",
    "    num = ord(char) - ord('a') + 1\n",
    "    return num\n",
    "  \n",
    "\n",
    "def num_to_char(num):\n",
    "\n",
    "    if num == 0:\n",
    "        return '_'\n",
    "    \n",
    "    char = chr(num + ord('a') - 1)    \n",
    "    return(char)\n",
    "\n",
    "\n",
    "def word_to_numlist(word):\n",
    "\n",
    "    num_list = [char_to_num(c) for c in word]\n",
    "    numlist = torch.tensor(num_list, dtype=torch.long)\n",
    "    return(numlist)\n",
    "\n",
    "\n",
    "def numlist_to_word(numlist):\n",
    "\n",
    "    chars = [num_to_char(num.item()) for num in numlist]\n",
    "    word =  ''.join(chars)\n",
    "    return(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d1936-fadb-4ddb-9027-3a75960aa6b1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font color='blue'>We convert letter into just numbers based on their aphabetical order, I claim that it is a very bad way to encode data such as letters to be fed into learning models, please write your explanation to or against my claim. If you are searching for reasons, the keyword 'categorical data' may be useful. Although the letters in our case are not treated as categorical data, the same reasons as for categorical data is applicable. Even if my claim is valid, at the end it won't matter due to something called \"embedding layers\" that we will use in our model. What is an embedding layer? What is it's purpose? Explain.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de3f69-cd06-4adc-8150-2522802f345a",
   "metadata": {
    "deletable": false,
    "id": "Ans2"
   },
   "source": [
    "**Answer** (to write answers edit this cell)\n",
    "\n",
    "The suggested claim is right because of following reasons\n",
    "\n",
    "1. It imposes a false ordinal relationship between characters.\n",
    "2. Characters are categorical, and numerical distance between indices can mislead the model into learning non-existent similarities.\n",
    "3. The model may learn spurious patterns due to the artificial ordering of characters.\n",
    "\n",
    "\n",
    "Embedding layer is a trainable lookup table that maps each integer to a dense vector of real numbers.\n",
    "\n",
    "Purpose of embedding layer : \n",
    "\n",
    "1. Converts discrete character IDs into continuous-valued vectors.\n",
    "2. Learns semantic relationships.\n",
    "3. Avoids sparse one-hot vectors, reducing memory usage and improving learning efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070a74-0f42-435d-a3ba-b38f0d1aaf3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Section 2: Implementing the Autocomplete model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb965e-afcd-41ae-86f0-2f3d4682e18b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will implement a RNN LSTM model. The [video tutorial](https://www.youtube.com/watch?v=tL5puCeDr-o) will be useful. Our model will be only one hidden layer, but feel free to sophisticate with more layers after the project for your own experiments.\n",
    "\n",
    "Our model will contain all the training and prediction methods as single package in a class (autocompleteModel) we will define and implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dfe311c-669d-4d58-a833-ae3970b6d271",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4976fc91-2c4e-497a-954e-9014dd31be5e",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "class autocompleteModel(nn.Module):\n",
    "\n",
    "    #Constructor\n",
    "    def __init__(self, alphabet_size, embed_dim, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #TODO\n",
    "\n",
    "        #Initialize the layers in the model:\n",
    "        #1 embedding layer, 1 - LSTM cell (hidden layer), 1 fully connected layer with linear activation\n",
    "        self.embed_layer = nn.Embedding(alphabet_size, embed_dim)\n",
    "        self.LSTM_cell = nn.LSTMCell(embed_dim, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, alphabet_size)\n",
    "\n",
    "\n",
    "    #Feedforward\n",
    "    def forward(self, character, hidden_state, cell_state):\n",
    "\n",
    "        #Perform feedforward in order\n",
    "        #1. Embed the input (one charcter represented by a number)\n",
    "        embedding = self.embed_layer(character)\n",
    "\n",
    "        #2. Feed the embedded output to the LSTM cell\n",
    "        hidden_state, cell_state = self.LSTM_cell(embedding, (hidden_state, cell_state))\n",
    "\n",
    "        #3. Feed the LSTM output to the fully connected layer to obtain the output\n",
    "        output = self.fc(hidden_state)\n",
    "\n",
    "        #4. return the output, and both the hidden state and cell state from the LSTM cell output        \n",
    "        return output, hidden_state, cell_state\n",
    "\n",
    "    #Intialize the first hidden state and cell state (for the start of a word) as zero tensors of required length.\n",
    "    def initial_state(self, batch_size=1):\n",
    "        h0 = torch.zeros(1, self.hidden_size)\n",
    "        c0 = torch.zeros(1, self.hidden_size)\n",
    "        \n",
    "        return (h0, c0)\n",
    "\n",
    "    #Train the model in epochs given the vocab, the training will be fed in batches of batch_size\n",
    "    def trainModel(self, vocab, epochs = 5, batch_size = 100, learning_rate = LEARNING_RATE, plot_status = False):\n",
    "\n",
    "        #Convert the model into train mode\n",
    "        self.train()\n",
    "\n",
    "        #Set the optimizer (ADAM), you may need to provide the model parameters  and learning rate\n",
    "        optimizer = optim.Adam(self.parameters(), learning_rate)\n",
    "\n",
    "        #Keep a log of the loss at the end of each training cycle.\n",
    "        loss_log = []\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "             #TODO: Shuffle the vocab list the start of each epoch\n",
    "            random.shuffle(vocab)\n",
    "\n",
    "            num_iter = len(vocab) // batch_size\n",
    "\n",
    "            batch_loss_list = []\n",
    "\n",
    "            accuracy = 0.0\n",
    "\n",
    "            for i in tqdm(range(num_iter)):\n",
    "\n",
    "                #TODO: Set the loss to zero, initialize the optimizer with zero_grad at the beginning of each training cycle.\n",
    "                batch_loss = 0\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                vocab_batch = vocab[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "                for word in vocab_batch:\n",
    "\n",
    "                    # Initialize the hidden state and cell state at the start of each word.\n",
    "                    hidden_state,cell_state = self.initial_state()\n",
    "\n",
    "                    # Convert the word into a tensor of number and create input and target from the word\n",
    "                    #Input will be the first WORD_SIZE - 1 charcters and target is the last WORD_SIZE - 1 charcters\n",
    "                    input_tensor = word_to_numlist(word[:WORD_SIZE-1])\n",
    "                    target_tensor = word_to_numlist(word[1:WORD_SIZE])\n",
    "\n",
    "                    #Loop through each character (as a number) in the word\n",
    "                    for c in range(WORD_SIZE - 1):\n",
    "                        #TODO: Feed the cth character to the model (feedforward) and comput the loss (use cross entropy in torch)\n",
    "                        output, hidden_state, cell_state = self.forward(input_tensor[c].unsqueeze(0), hidden_state, cell_state)\n",
    "                        loss = nn.functional.cross_entropy(output,target_tensor[c].view(1))\n",
    "                        batch_loss += loss\n",
    "                        accuracy += (output.argmax(dim=1) == target_tensor[c]).sum().item()\n",
    "                    \n",
    "\n",
    "                #TODO: Compute the average loss per word in the batch and perform backpropagation (.backward())\n",
    "                batch_loss /= len(vocab_batch)\n",
    "                batch_loss.backward()\n",
    "                \n",
    "                #TODO: Update model parameters using the optimizer\n",
    "                optimizer.step()\n",
    "\n",
    "                #Update the loss_log \n",
    "                batch_loss_list.append(batch_loss.item())\n",
    "\n",
    "            accuracy /= (13*batch_size*num_iter)\n",
    "\n",
    "            print(\"Epoch: \", e+1, \" Training loss per word : \", batch_loss.item(), f\" Training accuracy: {accuracy*100} % \")\n",
    "\n",
    "            loss_log.append(np.mean(batch_loss_list))\n",
    "\n",
    "        loss_log_processed = np.array(loss_log, dtype=np.float16)\n",
    "        \n",
    "        if plot_status:\n",
    "            # Plot a graph of the variation of the loss.\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(loss_log_processed)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss vs Epochs')\n",
    "            plt.show()\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    #Perform autocmplete given a sample of strings (typically 3-5 starting letters)\n",
    "    def autocomplete(self, sample):\n",
    "\n",
    "        #Convert the model into evaluation mode\n",
    "        self.eval()\n",
    "        completed_list = []\n",
    "\n",
    "        #TODO: In the following loop for each sample item initialize hidden and cell states, then predict the remaining characters\n",
    "        #You will have to convert the output into a softmax (you may use your softmax method from the last project) probability distribution, then use torch.multinomial \n",
    "        for i in tqdm(range(len(sample))):\n",
    "\n",
    "            literal = sample[i]\n",
    "\n",
    "            hidden_state,cell_state = self.initial_state()\n",
    "            input_tensor = word_to_numlist(literal)\n",
    "            predicted = literal\n",
    "\n",
    "            for p in range(len(literal) - 1):\n",
    "                init_input = input_tensor[p].unsqueeze(0)\n",
    "                nl, hidden_state, cell_state = self.forward(init_input, hidden_state, cell_state)\n",
    "            \n",
    "            init_input = input_tensor[-1].unsqueeze(0)\n",
    "            \n",
    "            for g in range(WORD_SIZE - len(literal)):\n",
    "                # generate\n",
    "                output, hidden_state, cell_state = self.forward(init_input, hidden_state, cell_state)\n",
    "                output_prob = nn.functional.softmax(output, dim = 1)\n",
    "                top_1 = torch.multinomial(output_prob, 1)[0]\n",
    "                \n",
    "                # prediction\n",
    "                pred_char = num_to_char(top_1)\n",
    "                predicted += pred_char\n",
    "                init_input = torch.tensor(char_to_num(pred_char)).unsqueeze(0)\n",
    "\n",
    "            completed_list.append(predicted)\n",
    "            \n",
    "        return(completed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b5489-b770-4519-b20c-4f2beebfb8f9",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Section 3: Using and evaluating the model\n",
    "\n",
    "(i) Initialize and train autocompleteModels using different embedding dimensions and hidden layer sizes. Use different learning rates, epochs, batch sizes. Train the best model you can.\n",
    "\n",
    "(ii) Evaluate it on different samples of partially filled in words to test your model. Eg: [\"univ\", \"math\", \"neur\", \"engin\"] etc.\n",
    "\n",
    "(iii) Set your best model, to the variable best_model. This model will be tested against random inputs (3-4 starting strings of common English words). **This will be the main contributor for your score in this project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38414f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('best_weights.pth', <http.client.HTTPMessage at 0x1b297b97950>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/SadeepRathnayaka/MA4144---Neural-Network-and-Fuzzy-Logic/main/best_wieghts.pth\"\n",
    "save_path = \"best_weights.pth\"\n",
    "urllib.request.urlretrieve(url, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac2ab12-0cc8-48d1-816f-ab8ec7ac23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11b1ecf-456c-408c-bb06-6f3ab91234f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autocompleteModel(\n",
       "  (embed_layer): Embedding(27, 64)\n",
       "  (LSTM_cell): LSTMCell(64, 256)\n",
       "  (fc): Linear(in_features=256, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = autocompleteModel(alphabet_size=27, embed_dim=64, hidden_size=256, num_layers=1) \n",
    "model.load_state_dict(torch.load(\"best_weights.pth\", weights_only=False))  # Optional: explicitly set weights_only=False\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e936498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 371.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Word : univ || Suggested Word : univilations\n",
      "Input Word : math || Suggested Word : mathematic\n",
      "Input Word : neur || Suggested Word : neurald\n",
      "Input Word : engin || Suggested Word : enginer\n",
      "Input Word : thi || Suggested Word : thinkpad\n",
      "Input Word : prod || Suggested Word : producers\n",
      "Input Word : auto || Suggested Word : automatic\n",
      "Input Word : reac || Suggested Word : reaction\n",
      "Input Word : stri || Suggested Word : strikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_seg = [\"univ\", \"math\", \"neur\", \"engin\",\"thi\", \"prod\", \"auto\", \"reac\", \"stri\"]\n",
    "predictions = model.autocomplete(word_seg)\n",
    "\n",
    "for a,prediction in zip(word_seg,predictions):\n",
    "    pred_ = prediction.replace(\"_\",\"\")\n",
    "    print(f'Input Word : {a} || Suggested Word : {pred_}')\n",
    "\n",
    "# print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37d4f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
